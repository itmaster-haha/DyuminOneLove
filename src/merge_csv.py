import pandas as pd
import logging
from pathlib import Path
import re
from datetime import datetime
import time

# --- –ü–∞—Ä—Å–µ—Ä—ã ---
from boarding_reader import read_boarding_csv
from sirena_reader import read_sirena_excel
from excel_reader import read_excel_dir
from xml_reader import read_xml_file
from json_reader import read_json_file
from read_yaml_file import read_yaml_file
from normalize_flight_table import normalize_flight_table
from name_utils import split_full_name, merge_duplicate_passengers

# ==========================================================
#                  –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø
# ==========================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)


#              –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø / –û–ë–©–ò–ï –£–¢–ò–õ–ò–¢–´

SCHEMA = [
    "FirstName","SecondName","LastName","PassengerSex","PassengerBirthDate",
    "TravelDoc","BookingCode","TicketNumber","Baggage",
    "DepartDate","DepartTime","FlightNumber","CodeShare",
    "DepartCity","ArrivalCity","DepartCode","ArrivalCode",
    "Airline","Fare","Seat","TrvCls","Meal","BonusProgramm","AgentInfo"
]


def normalize(df, mapping: dict | None = None) -> pd.DataFrame:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã DataFrame –ø–æ–¥ SCHEMA —Å –∞–≤—Ç–æ–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º '–±–∏—Ç—ã—Ö' –∫–æ–ª–æ–Ω–æ–∫."""
    if not isinstance(df, pd.DataFrame):
        logging.error(f" normalize –ø–æ–ª—É—á–∏–ª {type(df)}, –æ–∂–∏–¥–∞–ª—Å—è DataFrame. df={repr(df)[:200]}")
        return pd.DataFrame(columns=SCHEMA)

    if mapping is not None and not isinstance(mapping, dict):
        logging.error(f" mapping –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø: {type(mapping)} ‚Üí {repr(mapping)[:100]}")
        mapping = None

    # --- –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ ---
    try:
        if mapping:
            df = df.rename(columns=mapping)
    except Exception as e:
        logging.warning(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ–ª–æ–Ω–æ–∫: {e}")

    # --- –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ (Excel —á–∞—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –∏–ª–∏ tuple) ---
    try:
        df.columns = [str(c).strip() for c in df.columns]
    except Exception as e:
        logging.warning(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∏–º—ë–Ω –∫–æ–ª–æ–Ω–æ–∫: {e}")

    # --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ ¬´–±–∏—Ç—ã–µ¬ª –∫–æ–ª–æ–Ω–∫–∏ (–≥–¥–µ df[c] ‚Äî —Å—Ç—Ä–æ–∫–∞) ---
    bad_cols = []
    for c in df.columns:
        if not hasattr(df[c], "fillna") or isinstance(df[c], str):
            bad_cols.append(c)
    if bad_cols:
        logging.warning(f" –ù–∞–π–¥–µ–Ω—ã –±–∏—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {bad_cols}. –ü—Ä–µ–æ–±—Ä–∞–∑—É—é –∏—Ö –≤ Series.")
        for c in bad_cols:
            val = df[c]
            df[c] = pd.Series([val] * len(df))

    # --- fillna + astype ---
    try:
        df = df.fillna("").astype(str)
    except Exception as e:
        logging.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ fillna/astype(str): {e}")
        for c in df.columns:
            logging.error(f"  ‚Ä¢ {c}: {type(df[c])}")
        return pd.DataFrame(columns=SCHEMA)

    # --- –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ ---
    for col in SCHEMA:
        if col not in df.columns:
            df[col] = ""

    # --- –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ 
    try:
        df = df[SCHEMA]
    except Exception as e:
        logging.error(f" normalize: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–ª–æ–Ω–æ–∫: {e}")
        df = pd.DataFrame({col: df[col] if col in df.columns else "" for col in SCHEMA})

    return df


def clean_key(x):
    if pd.isna(x):
        return ""
    return re.sub(r"[^A-Z0-9]", "", str(x).strip().upper())


def normalize_date(val: str) -> str:
    """–ü—Ä–∏–≤–µ—Å—Ç–∏ –¥–∞—Ç—É –∫ ISO-—Ñ–æ—Ä–º–∞—Ç—É YYYY-MM-DD –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è."""
    if not val or pd.isna(val):
        return ""
    val = str(val).strip()
    if re.fullmatch(r"\d{8}", val):  
        return f"{val[:4]}-{val[4:6]}-{val[6:]}"
    for fmt in ("%d.%m.%Y", "%Y-%m-%d", "%Y/%m/%d"):
        try:
            return datetime.strptime(val, fmt).strftime("%Y-%m-%d")
        except ValueError:
            continue
    return val



#                  –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ò–ú–Å–ù
POSSIBLE_FULLNAME_COLS = ["FullName", "PassengerName", "PAX", "Name"]


def recover_names_from_fullname(df: pd.DataFrame) -> pd.DataFrame:
    fn_col = next((c for c in POSSIBLE_FULLNAME_COLS if c in df.columns), None)
    if not fn_col:
        return df
    try:
        need_split = (
            df.get("FirstName", pd.Series([""] * len(df))).fillna("") == ""
        ) & (df[fn_col].fillna("").str.strip() != "")
        if need_split.any():
            parsed = split_full_name(
                df.loc[need_split, [fn_col]].rename(columns={fn_col: "FullName"}), "FullName", ""
            )
            for c in ["FirstName", "SecondName", "LastName"]:
                if c not in df.columns:
                    df[c] = ""
                df.loc[need_split & (df[c].fillna("") == ""), c] = parsed[c]
    except Exception as e:
        logging.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ FullName: {e}")
    return df

def merge_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:
    merged = pd.DataFrame()
    base = sorted(set([re.sub(r'_[xy]$', '', c) for c in df.columns]))
    for col in base:
        col_x, col_y = f"{col}_x", f"{col}_y"
        if col_x in df.columns and col_y in df.columns:
            merged[col] = df[col_x].fillna("").astype(str)
            yvals = df[col_y].fillna("").astype(str)
            merged[col] = merged[col].where(merged[col] != "", yvals)
        elif col_x in df.columns:
            merged[col] = df[col_x]
        elif col_y in df.columns:
            merged[col] = df[col_y]
        elif col in df.columns:
            merged[col] = df[col]
        else:
            merged[col] = ""
    return merged



#                     –≠–¢–ê–ü 1 ‚Äî –ü–ê–°–°–ê–ñ–ò–†–´

def build_passenger_block(paths: dict, output: Path):
    start = time.time()
    logging.info(" –≠—Ç–∞–ø 1: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–∞—Å—Å–∞–∂–∏—Ä—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")

    dfs = []
    mappings = {
        "boarding": {
            "PassengerFirstName": "FirstName",
            "PassengerSecondName": "SecondName",
            "PassengerLastName": "LastName",
            "PassengerDocument": "TravelDoc",
            "PassengerSex": "PassengerSex",
            "PassengerBirthDate": "PassengerBirthDate",
            "FlightDate": "DepartDate",
            "FlightTime": "DepartTime",
            "Destination": "ArrivalCity",
        },
        "yaml": {"Class": "TrvCls", "Status": "CodeShare"},
    }

    readers = {
        "boarding": read_boarding_csv,
        "sirena": read_sirena_excel,
        "excel": read_excel_dir,
        "xml": read_xml_file,
    }

    for name, reader in readers.items():
        try:
            df = reader(paths[name])
            logging.info(f" –ò—Å—Ç–æ—á–Ω–∏–∫: {name:8s} | —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö = {type(df)}, —Ä–∞–∑–º–µ—Ä = {getattr(df, 'shape', 'n/a')}")

            if not isinstance(df, pd.DataFrame):
                logging.warning(f" {name} –≤–µ—Ä–Ω—É–ª {type(df)}, –æ–∂–∏–¥–∞–µ—Ç—Å—è DataFrame. –ò—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–æ–ø—É—â–µ–Ω.")
                continue
            if df.empty:
                logging.warning(f" {name} –ø—É—Å—Ç.")
                continue

            # ---  –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–º—ë–Ω ---
            has_fullname = "FullName" in df.columns
            has_names = any(c in df.columns for c in ["FirstName", "LastName"])
            if has_fullname and not has_names:
                logging.info(f" {name}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω FullName –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∏–º—ë–Ω ‚Äî –≤—ã–ø–æ–ª–Ω—è—é —Ä–∞–∑–±–∏–µ–Ω–∏–µ...")
                df = recover_names_from_fullname(df)
                try:
                    sample = df.loc[:, ["FullName", "FirstName", "LastName"]].head(3).to_dict(orient="records")
                    logging.info(f" –ü—Ä–∏–º–µ—Ä —Ä–∞–∑–±–∏–µ–Ω–∏—è: {sample}")
                except Exception:
                    pass

            df = normalize(df, mappings.get(name))

            if "FirstName" in df.columns:
                df = df[df["FirstName"].astype(str).str.strip() != ""]

            dfs.append(df)
            logging.info(f" {name}: {len(df)} —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
        except Exception as e:
            logging.error(f" –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {name}: {e}")

    if not dfs:
        logging.error(" –ù–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ DataFrame –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è.")
        return pd.DataFrame()

    merged = pd.concat(dfs, ignore_index=True)
    logging.info(f" –î–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º: {len(merged)} —Å—Ç—Ä–æ–∫")

    suspicious_log = Path("./data/processed/suspicious_docs.log")
    merged = merge_duplicate_passengers(merged, suspicious_log_path=suspicious_log)

    merged.to_csv(output, sep=";", index=False, encoding="utf-8")
    logging.info(f" –ü–∞—Å—Å–∞–∂–∏—Ä—Å–∫–∏–π –±–ª–æ–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output} ({len(merged)} —Å—Ç—Ä–æ–∫, –≤—Ä–µ–º—è: {time.time()-start:.1f}s)")
    return output



#              –≠–¢–ê–ü 2 ‚Äî –î–û–ë–ê–í–ò–¢–¨ –†–ï–ô–°–´ + JSON/YAML
def attach_flights(passengers: Path, flights: Path, json_path: Path, yaml_path: Path, output: Path):
    start = time.time()
    logging.info(" –≠—Ç–∞–ø 2: –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–π—Å–æ–≤ (–≤–∫–ª—é—á–∞—è JSON –∏ YAML)...")

    p = pd.read_csv(passengers, sep=";", dtype=str)
    logging.info(f" –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤: {len(p)}")

    f = normalize_flight_table(flights)
    f["DepartDate"] = f["DepartDate"].apply(normalize_date)
    logging.info(f" –û—Å–Ω–æ–≤–Ω–æ–π flights.csv: {len(f)} —Å—Ç—Ä–æ–∫")

    json_df = read_json_file(json_path)
    yaml_df = read_yaml_file(yaml_path)
    json_df = normalize(json_df)
    yaml_df = normalize(yaml_df)

    for d in (json_df, yaml_df):
        if "DepartDate" in d.columns:
            d["DepartDate"] = d["DepartDate"].apply(normalize_date)

    f_all = pd.concat([f, json_df, yaml_df], ignore_index=True)
    logging.info(f" –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –±–ª–æ–∫ —Ä–µ–π—Å–æ–≤: {len(f_all)} —Å—Ç—Ä–æ–∫ (–∏–∑ CSV+JSON+YAML)")

    for df in (p, f_all):
        for c in ["TravelDoc", "TicketNumber", "FlightNumber", "DepartDate"]:
            if c in df.columns:
                df[c] = df[c].fillna("").apply(clean_key)

    key_cols = ["FlightNumber", "DepartDate"]
    merged = p.merge(f_all, on=key_cols, how="left")
    merged = merge_duplicate_columns(merged)
    matched = merged["Airline"].notna().sum()
    logging.info(f" –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ä–µ–π—Å–∞–º: {matched}/{len(merged)} ({matched/len(merged):.1%})")

    merged.to_csv(output, sep=";", index=False, encoding="utf-8")
    logging.info(f" –ü–∞—Å—Å–∞–∂–∏—Ä—ã + —Ä–µ–π—Å—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output} (–≤—Ä–µ–º—è: {time.time()-start:.1f}s)")
    return output



#                     –≠–¢–ê–ü 3 ‚Äî –§–ò–ù–ê–õ
def clean_columns(input_path: Path, output: Path):
    start = time.time()
    logging.info("üßπ –≠—Ç–∞–ø 3: –æ—á–∏—Å—Ç–∫–∞ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫...")
    df = pd.read_csv(input_path, sep=";", dtype=str)
    df.columns = [c.replace("flights_", "").replace("sirena_", "").replace("boarding_", "")
                    .replace("excel_", "").replace("xml_", "").replace("json_", "")
                    .replace("yaml_", "") for c in df.columns]
    df = merge_duplicate_columns(df)
    df.to_csv(output, sep=";", index=False, encoding="utf-8")
    logging.info(f" –§–∏–Ω–∞–ª—å–Ω—ã–π CSV –≥–æ—Ç–æ–≤: {output} ({len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫, {time.time()-start:.1f}s)")
    return output

#                     PIPELINE

if __name__ == "__main__":
    base = Path("./data/processed")
    base.mkdir(parents=True, exist_ok=True)

    paths = {
        "boarding": "./data/raw/BoardingData.csv",
        "sirena": "./data/raw/Sirena-export-fixed.xlsx",
        "excel": "./data/raw/YourBoardingPassDotAero",
        "xml": "./data/raw/PointzAggregator-AirlinesData.xml",
        "json": "./data/raw/FrequentFlyerForum-Profiles.json",
        "yaml": "./data/raw/SkyTeam-Exchange.yaml",
        "flights": "./data/processed/trial_flights.csv",
    }

    p1 = build_passenger_block(paths, base / "merged_passengers.csv")
    p2 = attach_flights(p1, paths["flights"], paths["json"], paths["yaml"], base / "merged_passengers_flights.csv")
    p3 = clean_columns(p2, base / "merged_all_sources.csv")

    # --- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ---
    df = pd.read_csv(p3, sep=";", dtype=str)
    logging.info(" --- –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–Å–¢ ---")
    logging.info(f" –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df):,}")
    missing_airline = (df["Airline"].fillna("") == "").sum()
    logging.info(f" –ë–µ–∑ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–π—Å–µ: {missing_airline} ({missing_airline/len(df):.1%})")
    missing_name = (df["FirstName"].fillna("") == "").sum()
    logging.info(f" –ë–µ–∑ –∏–º–µ–Ω–∏: {missing_name} ({missing_name/len(df):.1%})")
    logging.info(" –ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω!")
